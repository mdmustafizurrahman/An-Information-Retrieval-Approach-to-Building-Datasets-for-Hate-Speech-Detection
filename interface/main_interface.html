<script src="https://assets.crowd.aws/crowd-html-elements.js"></script>

<style>
  .key-phrase {
    color: green;
  }

  ::selection {
  color: red;
  background: yellow;
}
</style>

<title> Annotate Hate Speech </title>

   <div id="additionalQuestions" style="margin-top: 20px">

      <b> What is Hate Speech? </b> Hate speech contains <u>derogatory language</u> and/or <u>incites violence</u> against an individual or group of people <u>on the basis of group identity</u> (e.g., race, religion, skin color, sexual identity, gender identity, ethnicity, disability, national origin, etc.).

               <br> <br>
        <b>What is NOT Hate Speech?</b>
        <ul>
<li>Derogatory language that is NOT in relation to any group identity (“John is stupid.”) should NOT be labeled as hateful.
            <li> <b>Pornography</b>. Language related to sexuality-related content should NOT be labeled as hateful.
        </ul>


      </p>
       <p>For complete instructions, please refer to the <b>instruction tab</b>. </p>
      </div>


<crowd-entity-annotation
  name="key-phrases"

  header="Identify hate speech: highlight 1) inciting violence, 2) derogatory language, and 3) the intended target."
  labels="[
    {'label': 'DEROGATORY LANGUAGE', 'shortDisplayName': 'DEROGATORY LANGUAGE', 'fullDisplayName': 'DEROGATORY LANGUAGE'},
    {'label': 'INCITE VIOLENCE', 'shortDisplayName': 'INCITE VIOLENCE', 'fullDisplayName': 'INCITE VIOLENCE'},

    {'label': 'GROUP:BODY', 'shortDisplayName': 'GROUP:BODY', 'fullDisplayName': 'GROUP:BODY'},
    {'label': 'GROUP:GENDER', 'shortDisplayName': 'GROUP:GENDER', 'fullDisplayName': 'GROUP:GENDER'},
    {'label': 'GROUP:IDEOLOGY', 'shortDisplayName': 'GROUP:IDEOLOGY', 'fullDisplayName': 'GROUP:IDEOLOGY'},

    {'label': 'GROUP:RACE', 'shortDisplayName': 'GROUP:RACE', 'fullDisplayName': 'GROUP:RACE'},
    {'label': 'GROUP:RELIGION', 'shortDisplayName': 'GROUP:RELIGION', 'fullDisplayName': 'GROUP:RELIGION'},

    {'label': 'GROUP:SEXUAL_ORIENTATION', 'shortDisplayName': 'GROUP:SEXUAL ORIENTATION', 'fullDisplayName': 'GROUP:SEXUAL ORIENTATION'},
    {'label': 'GROUP:OTHER', 'shortDisplayName': 'GROUP:OTHER', 'fullDisplayName': 'GROUP:OTHER'}
    ]"
  text="${tweettext}"

>



      <div id="anotherOne" style="margin-top: 20px">
      <br>

      <p>
          <b>Step 1:</b> Highlight any words or phrases in the above tweet INCITING VIOLENCE. <br>
          <ul>
          <li> <b>NOTE</b>: please highlight only key portions of the tweet; highlighting the entire tweet is not helpful and will result in rejection.

          </ul>
          <b>Step 2:</b> Highlight any DEROGATORY LANGUAGE in the above tweet on the basis of group identity. <br>
          <ul>
          <li> <b>NOTE</b>: please highlight only key portions of the tweet; highlighting the entire tweet is not helpful and will result in rejection.

          </ul>


          <b>Step 3:</b> If the tweet IMPLICITLY incites violence or denigrates an individual or group on the basis of group identity, use the drop-down selection here to indicate this.
          <select id="ImplicithateType" name="ImplicithateType" required>
          <option value=""></option>
          <option value="NONE">NONE</option>
          <option value="DEROGATORY LANGUAGE">DEROGATORY LANGUAGE</option>
          <option value="INCITE VIOLENCE">INCITE VIOLENCE</option>
            </select>
         <br>
          <b>Step 4:</b> Highlight the INTENDED TARGET in the tweet above (if the target is explicit).
          <ul>
          <li> <b>NOTE</b>: please highlight only key portions of the tweet; highlighting the entire tweet is not helpful and will result in rejection.

          </ul>
           If the target is implicit, write the INTENDED TARGET in the text box here. In case there is no implicit target, write "NONE" in the box here          <input type="text" id="Implicittaret" name="Implicittaret" value="" required> <br>

          <ul>

               <li>   <b>NOTE:</b> If you have highlighted any terms related to derogatory language or inciting violence but have not highlighted any terms related to the targeted group or have not mentioned the target in the textbox under step 4 or vice versa, especially when you think that the tweet is hateful, your task will be rejected.
              </ul>
          </p>

      <label for="categoryGroup"><b>Step 5: </b> Use the drop-down selection here to identify the type of group targeted (explicit or implicit). </label>
          <select id="categoryGroup" name="categoryGroup" required>
          <option value=""></option>
          <option value="NONE">NONE</option>

          <option value="GROUP:BODY">GROUP:BODY</option>
          <option value="GROUP:GENDER">GROUP:GENDER</option>
          <option value="GROUP:IDEOLOGY">GROUP:IDEOLOGY</option>
          <option value="GROUP:RACE">GROUP:RACE</option>
          <option value="GROUP:RELIGION">GROUP:RELIGION</option>
          <option value="GROUP:SEXUAL_ORIENTATION">GROUP:SEXUAL ORIENTATION</option>
          <option value="GROUP:OTHER">GROUP:OTHER</option>


        </select>

      <br>


      <p> <b>Step 6:</b> Based on your answers to the above steps, do you believe the tweet is hateful?
      <input type="radio" id="yes" name="hateornot" value="yes" required>
      <label for="yes">yes</label>
      <input type="radio" id="no" name="hateornot" value="no">
      <label for="no">no</label>

      </p>

      <p><b>Step 7: </b> If you DO NOT find anything to highlight that is EXPLICIT regarding DEROGATORY LANGUAGE or INCITING VIOLENCE and INTENDED TARGET, please select the "No entities to label" checkbox at the bottom right beside the submit button.

      </p>


      <p> <b> Rationale [OPTIONAL]</b>: We welcome any additional explanation of your labeling decisions you would like to provide. </p>

      <textarea rows = "5" cols = "50" name = "description"></textarea>

      <p> <b> Comments [OPTIONAL]</b>: Do you have any feedback on how we can improve our task? Thanks! </p>

      <textarea rows = "5" cols = "50" name = "comments"></textarea>

      <br>

      <table border="1">
      <caption> <b>GROUP CATEGORIES</b></caption>
        <tr>
        <td style="width:70px">  GROUP:BODY </td>
        <td>  Relative to body, such as slender or overweight, tall or short, etc. </td>
        </tr>

        <tr>
        <td style="width:70px">  GROUP:GENDER </td>
        <td>  Relative to gender (e.g., male, female, transgender) or ideology related to gender (e.g., feminist).  For example, sexism against women. </td>
        </tr>



        <tr>
        <td style="width:70px">
GROUP:IDEOLOGY  </td>
        <td>
Relative to a person’s ideology, such as liberal, moderate, conservative, etc. </td>
      </tr>


        <tr>
        <td style="width:70px">  GROUP:RACE </td>
        <td>
Relative to race, ethnicity, and/or origin, such as Caucasian, African American, Hispanic, Chinese, Mexican, etc. </td>
       </tr>

        <tr>
        <td style="width:70px">  GROUP:RELIGION </td>
        <td>  Relative to religion, such as Christian/Chistianity, Muslim/Islam, Budhist/Buddhism, etc.
        </td>
        </tr>



        <tr>
        <td style="width:70px">
GROUP:SEXUAL ORIENTATION </td>
        <td>   Relative to sexual orientation, such as heterosexual, homosexual, bisexual, polygamy, etc. </td>
       </tr>


        <td style="width:70px">  GROUP:OTHER </td>
        <td>  Relative to any other group identity, such as physical or mental disabilities, dietary preferences/restrictions, occupation, economic class, etc. Also used when group is unknown (e.g., “people like John”, where it is unclear which group is being identified.)</td>
        </tr>


      </table>

  </div>

    <short-instructions>

      <b>Disclaimer:</b> <p> Our research seeks to reduce the spread of hate speech on social media by training computer programs to automatically detect hate speech. To accomplish this, we ask human annotators to read Tweets and label hate speech. We understand that this labeling task requires content that can be disturbing to read. If you prefer to return this task rather than work on it, we understand. In general, if you ever experience mental or emotional distress, please know that help is available online.  Helplines include https://suicidepreventionlifeline.org/ in the USA and http://suicide.org/international-suicide-hotlines.html internationally. For additional reading on this subject, please consult our research article, “The Psychological Well-Being of Content Moderators” (<a href="https://www.ischool.utexas.edu/~ml/papers/steiger-chi21.pdf">https://www.ischool.utexas.edu/~ml/papers/steiger-chi21.pdf)</a>.</p>


      <br><br>
            <b>What is Hate Speech?</b> Hate speech contains <u>derogatory language</u> and/or <u>incites violence</u> against an individual or group of people <u>on the basis of group identity</u> (e.g., race, religion, skin color, sexual identity, gender identity, ethnicity, disability, national origin, etc.).

        <br> <br>
        <b>What is NOT Hate Speech?</b>
        <ul>
<li>Derogatory language that is NOT in relation to any group identity (“John is stupid.”) should NOT be labeled as hateful.
            <li> <b>Pornography</b>. Language related to sexuality-related content should NOT be labeled as hateful.
        </ul>

<br><br>
<b>Task</b> This task asks you to identify hateful tweets. The typical steps involved in this task are as follows:
<br><br>

<b>Step 1: Does the tweet INCITE VIOLENCE (for any reason)?</b>. If so, is this explicit or implicit?
<ul>
<li><b>Explicit:</b> if language inciting violence is explicit in the tweet, highlight the corresponding word(s) or phrase(s) and select the category INCITES VIOLENCE.
<li><b>Implicit:</b> If the tweet implicitly incites violence, select INCITES VIOLENCE from the drop-down selection under Step 3.
</ul>

<b>Step 2: Does the tweet have DEROGATORY LANGUAGE </b> that is targeted towards an individual or group on the basis of a group identity? If so, is this explicit or implicit?
<ul>
<li><b>Explicit:</b> if the derogatory language on the basis of a group identity, is explicit in the tweet, highlight the corresponding word(s) or phrase(s) and select the category DEROGATORY LANGUAGE.
<li><b>Implicit:</b> If the tweet implicitly denigrates an individual or group on the basis of a group identity, select DEROGATORY LANGUAGE from the drop-down selection under Step 3.
</ul>
<b>Step 3:</b> If the tweet IMPLICITLY incites violence or denigrates an individual or group on the basis of group identity, use the drop-down selection here to indicate this.
<br><br>
<b>Step 4: If either of Step 1, Step 2 or Step 3 is TRUE, does the tweet have a  targeted group</b>? If there is a targeted group, is that explicit or implicit?
<ul>
<li><b>Explicit:</b> If the targeted group identity is explicit, highlight and categorize any word(s) or phrase(s) in the tweet that identify the targeted group. For example: if hate is expressed toward “white people”, highlight “white people” and select the category “GROUP:RACE”.

<li><b>Implicit:</b> If the targeted group is implicit, write the INTENDED TARGET in the text box under Step 4.
</ul>

<br> <br>
<b>Step 5.</b> Use the drop-down selection here to identify the type of group targeted (explicit or implicit).
<br><br>

<br> <br>
<b> Step 6</b>: Based on your answers to the above steps, do you believe the tweet is hateful? Select your option from the radio button.
<br><br>

<br> If you DO NOT find anything to highlight that is EXPLICIT regarding DEROGATORY LANGUAGE or INCITING VIOLENCE and INTENDED TARGET,
please select the "No entities to label" checkbox at the bottom right beside the submit button.
Please ignore the submit button in the instruction tab. Use the submit button on the main task tab.
<br> <br>

<b>Special case.</b> Sometimes a word or phrase is both DEROGATORY LANGUAGE and identifies the targeted group at the same time. Because
each highlight can only have a single category associated with it, and you have already highlighted the word(s) or
phrase(s) as DEROGATORY LANGUAGE, use the GROUP textbox to write in the name of the targeted group, and use the GROUP
drop-down selection option below to select the appropriate group identity. For example, if the word
        “cracker” was used as a DEROGATORY term for Caucasians, please 1) use the additional text box to write in “Caucasians” and 2) use the additional drop-down selection to select the category “GROUP:RACE”.



<br><br> <b>Clarifying Example for Target</b>:
Imagine a hateful tweet that says “Attack <b>X</b>”, where <b>X</b> is:
<ul>

<li><b>X</b>=”sexist men like John Smith”. The target is a group: ”sexist men like John Smith”. Highlight this group with category “GROUP:GENDER”
<li><b>X</b>=”people like John Smith”. The target is a group (“people like John Smith”), but it is unclear what group identity is being referred to.  Highlight the group and select category “GROUP:OTHER”.

</ul>

<br><br> <b>Clarifying Examples for implicit hateful tweet</b>:
      <br><b>Example Tweet</b>:  Six million wasn’t enough (6MWE)
      <br><b>Incites Violence:</b>: yes, implicit, select INCITES VIOLENCE from the “Hate Type” drop-down selection.
      <br><b>Derogatory language:</b> no

      <br><b>Targeted Group</b>: implicit, write “Jewish people” and select GROUP:Religion

      <br><b>Explanation:</b> this tweet implicitly refers to the six million Jewish people killed during the Holocaust and thus incites violence.  Reference: <a href="https://www.snopes.com/fact-check/proud-boy-6mwe/">https://www.snopes.com/fact-check/proud-boy-6mwe/</a>.



    <!--
      <p><b>Task:</b> Given a tweet, does it

      <br> i) contain any derogatory words
      <br> or ii) incite violence which is directed  towards an individual or a specific group of people in reference to the identity of the individual or group?
      <br> <br>  If so, at first, highlight the words reflecting derogatory words or violence.
      <br> <br> Next, highlight the words reflecting the targeted explicit individual or explicit group.
      <br> <br> For explicit group, select any of these: i) Sexism, ii) Body, iii) Origin, iv) Homophobia, v) Racism, vi) Ideology, vii) Religion, viii) Health, ix) other.
      <br> <br> If the individual or groups targeted in the tweet above is implicit, please report that in the second text box.
      <br> <br> Furthermore, if a single word represents both derogatory term and group, highlight only words refelecting groups.
      </p>


      <table border="1">
        <tr>
        <td style="width:70px">  Sexism </td>
        <td>  Derogatory terms based on gender includes women. </td>
        </tr>
        <td style="width:70px">  Body </td>
        <td>  Derogatory terms based on on body, such as fat, thin, tall or short people. </td>
        </tr>
        <td style="width:70px">  Origin </td>
        <td>  Derogatory terms based on the place of origin. For examples, Mexican, Chinese, etc. </td>
       </tr>

        <td style="width:70px">  Homophobia </td>
        <td>  Derogatory terms based on sexual orientation. For instance, LGBTQ+, etc.</td>
       </tr>

        <td style="width:70px">  Racism </td>
        <td>  Derogatory terms based on race and ethnicity. For example, black people. </td>
      </tr>

        <td style="width:70px">  Ideology </td>
        <td>  Derogatory terms based on a person’s ideas, such as feminist or left wing ideology </td>
        </tr>

        <td style="width:70px">  Religion </td>
        <td>  Derogatory terms based on Religion. For example, Islam. </td>
        </tr>

       <td style="width:70px">  Health </td>
        <td>  Derogatory terms based on health conditions, such as against disabled people</td>
        </tr>


       <td style="width:70px">  Other-Lifestyle </td>
        <td>  Derogatory terms based on life habits, such as vegetarianism.</td>
        </tr>


      </table>
      -->


      Please read the following examples carefully to understand how a tweet looks like under each category.
      <p>

      <br><b>Example Tweet 1</b>: “Poor white kids being forced to treat apes and parasites as their equals.”
      <br><b>Derogatory language</b>: “apes” and “parasites”
      <br><b>Targeted Group</b>: GROUP:RACE


      <br><b>Rationale</b>: In this tweet, the terms “apes” and “parasites” are used to imply that darker skinned people are not equal to caucasians (white people). This degrades the group composed of those with dark skin based on an identifying characteristic, namely, their skin color.

      <br>
      <br><b>Example Tweet 2</b>: “Islam is a false religion however unlike some other false religions it is crude and appeals to crude people such as arabs”
      <br><b>Derogatory language</b>: “Crude”, “false religions”
      <br><b>Targeted Group</b>: GROUP:RELIGION

      <br>
      <br><b>Example Tweet 3</b>: “Illegal immigration is a cancer which if not eliminated will bring the downfall of Europe and European culture”
      <br><b>Derogatory language</b>:  “cancer”
      <br><b>Targeted Group</b>: GROUP:RACE

      <br>
      <br><b>Example Tweet 4</b>: “I know of Donald Trump and subsequently have hated him for a long time”
      <br><b>Derogatory language</b>: NOTHING
      <br><b>Targeted Group</b>: NONE


      <!--
      <br>
      <br>Besides, it must be emphasized that the presence of pejorative language in a sentence cannot systematically be considered sufficient evidence to confirm the existence of hate speech. The use of “fag” in the following tweet:


      <br><br><b>Example Tweet 5</b>: Six million is not enough
      <br><b>Derogatory language</b>: NOTHING
      <br><b>Individual or Group</b>: Implicit towards Jews religion group


      <br><br><b>Example Tweet 6</b>: “Trump is very comfortable attacking Latinos, women and blacks because he believes white men are superior in every way”

      <br><b>Derogatory language</b>: NOTHING
      <br><b>Individual or Group</b>: Racism, Origin, Sexism
      -->
 </short-instructions>

<full-instructions>

</full-instructions>

</crowd-entity-annotation>

<script>
  document.addEventListener('all-crowd-elements-ready', myFunction);

 document.addEventListener('all-crowd-elements-ready', myFunction2);

  function myFunction() {
       document
      .querySelector('crowd-entity-annotation')
      .shadowRoot
      .querySelector('crowd-form')
      .form
      .prepend(additionalQuestions);
  }

  function myFunction2() {
       document
      .querySelector('crowd-entity-annotation')
      .shadowRoot
      .querySelector('crowd-form')
      .form
      .appendChild(anotherOne);
  }


</script>
